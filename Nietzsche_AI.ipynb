{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nietzsche-AI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjNJkvrkXUodebwiTNy2kG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matchingg/nietzsche-ai/blob/main/Nietzsche_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "RcA1haMIoDTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "j8oI6yzmoEzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import File"
      ],
      "metadata": {
        "id": "9Fe4o8Jqo6UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')"
      ],
      "metadata": {
        "id": "ayaQi59BoR5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4d31d5-1d70-4779-ff4c-ca018c631c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 1s 2us/step\n",
            "614400/600901 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read File"
      ],
      "metadata": {
        "id": "U3CTmbM6o9o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file,'rb').read()\n",
        "text = text.decode(encoding='utf-8')\n",
        "text = text.replace('\\n', ' ')\n",
        "print('Total number of characters is:', len(text))\n",
        "print('The first 100 characters are as follows:\\n', text[:100])"
      ],
      "metadata": {
        "id": "rd1NWFDQo2un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d70bac-f7a2-410f-8c94-430ab709272d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters is: 600893\n",
            "The first 100 characters are as follows:\n",
            " PREFACE   SUPPOSING that Truth is a woman--what then? Is there not ground for suspecting that all ph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing Text"
      ],
      "metadata": {
        "id": "4c5Dw3SuqIbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print('The number of unique characters is', len(vocab))\n",
        "print('A slice of the unique characters set:\\n', vocab[:10])"
      ],
      "metadata": {
        "id": "2y_ORuSKpNkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ead717e-9efa-45ee-9b07-eaa7a18d4dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique characters is 83\n",
            "A slice of the unique characters set:\n",
            " [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "P1SVyO4yqY_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Dataset"
      ],
      "metadata": {
        "id": "LKl0uCrqrDAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "seq_length = 100\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "h5AW4VL8rEfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "9aKM8PT7rXVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "7kCkciPTrpaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad87844-7183-4ee3-fd66-6acc11a13afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Model"
      ],
      "metadata": {
        "id": "XLFysyGIr5tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "ZZCR7B8qr2ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "          tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "          tf.keras.layers.Dense(vocab_size)])\n",
        "  return model"
      ],
      "metadata": {
        "id": "Ru4LZDa9sJJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size = len(vocab),\n",
        "                    embedding_dim=embedding_dim,\n",
        "                    rnn_units=rnn_units,\n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "E6y-JwnAszfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d513d59-f90b-42a5-f9c3-40c4c804e928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           21248     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 83)            85075     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,044,627\n",
            "Trainable params: 4,044,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling and Training"
      ],
      "metadata": {
        "id": "YvJRyTjEtgTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "DGLABtbDs_90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "metadata": {
        "id": "fzIdq_blt-KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVdTMl9GuTmd",
        "outputId": "8e50b87f-78fa-4b7a-84f2-f6b0dfd7e565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "92/92 [==============================] - 15s 53ms/step - loss: 2.9371\n",
            "Epoch 2/30\n",
            "92/92 [==============================] - 6s 53ms/step - loss: 2.2604\n",
            "Epoch 3/30\n",
            "92/92 [==============================] - 6s 53ms/step - loss: 1.9962\n",
            "Epoch 4/30\n",
            "92/92 [==============================] - 6s 54ms/step - loss: 1.7813\n",
            "Epoch 5/30\n",
            "92/92 [==============================] - 6s 54ms/step - loss: 1.6180\n",
            "Epoch 6/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.5042\n",
            "Epoch 7/30\n",
            "92/92 [==============================] - 6s 54ms/step - loss: 1.4228\n",
            "Epoch 8/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.3610\n",
            "Epoch 9/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.3127\n",
            "Epoch 10/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.2702\n",
            "Epoch 11/30\n",
            "92/92 [==============================] - 6s 57ms/step - loss: 1.2313\n",
            "Epoch 12/30\n",
            "92/92 [==============================] - 6s 56ms/step - loss: 1.1950\n",
            "Epoch 13/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.1594\n",
            "Epoch 14/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.1240\n",
            "Epoch 15/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.0893\n",
            "Epoch 16/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 1.0541\n",
            "Epoch 17/30\n",
            "92/92 [==============================] - 6s 54ms/step - loss: 1.0139\n",
            "Epoch 18/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.9773\n",
            "Epoch 19/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.9381\n",
            "Epoch 20/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.8994\n",
            "Epoch 21/30\n",
            "92/92 [==============================] - 6s 56ms/step - loss: 0.8570\n",
            "Epoch 22/30\n",
            "92/92 [==============================] - 6s 56ms/step - loss: 0.8146\n",
            "Epoch 23/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.7742\n",
            "Epoch 24/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.7343\n",
            "Epoch 25/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.6947\n",
            "Epoch 26/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.6598\n",
            "Epoch 27/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.6262\n",
            "Epoch 28/30\n",
            "92/92 [==============================] - 6s 54ms/step - loss: 0.5925\n",
            "Epoch 29/30\n",
            "92/92 [==============================] - 6s 55ms/step - loss: 0.5670\n",
            "Epoch 30/30\n",
            "92/92 [==============================] - 6s 56ms/step - loss: 0.5444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating New Text"
      ],
      "metadata": {
        "id": "EA0YKI1X3Dc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4sQx1cGdubLK",
        "outputId": "2bc71b9e-90c8-4d99-bb1a-5b5afe619a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pk8_XHIp15ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model.pkl', 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UYCvfecNFQJ",
        "outputId": "937aa5ad-19e6-4d2e-aeb0-45f0070d5aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://40586371-d99f-4ad8-814e-ea67b9a76532/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://40586371-d99f-4ad8-814e-ea67b9a76532/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f55d6174d90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "-GsY_JjT9yxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, num_generate, temperature, start_string):\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  generated_text = start_string + ''.join(text_generated)\n",
        "  generated_text = generated_text[:generated_text.rfind(' ')] + '.'\n",
        "  generated_text = ''.join(i for i in generated_text if not i.isdigit())\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "qsE1TbXC2OxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(model, num_generate=500, temperature=1, start_string='Plato')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpZgSUF53AwW",
        "outputId": "dc682c86-fd53-4572-b059-434e7a598335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platon of \"Trificing, and melianity generally estimate the thing is shy no means eternal, for their protection and passions within associated with palidn tially suffers from nature, for the end, however, though he accepts, not as he present may revenges it and its ultimate conception hitherto. There is not our duties as clear as the unselfish man: niner, and the French Revolution (in the case of SPPICITACE for himself; the same, so that even Greek life as frle of worship results universally bare.\n"
          ]
        }
      ]
    }
  ]
}